{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlYjfaGHem2V"
      },
      "source": [
        "# ML Pipelines with ZenML\n",
        "\n",
        "***Key Concepts:*** *ML Pipelines, Steps*\n",
        "\n",
        "In this notebook, we will learn how to easily convert existing ML code into ML pipelines using ZenML."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning in production consists of wide variety of tasks ranging from experiment tracking to orchestration, from model deployment to monitoring, from drift detection to feature stores and much, much more than that. Even though there are already some seemingly well-established solutions for these tasks, it can become increasingly difficult to establish a running production system in a reliable and modular manner once all these solutions are brought together. This is a problem which is especially critical when switching from research setting to a production setting. Due to a lack of standards, the time and resources invested in proof of concepts frequently go completely to waste, because the initial system cannot easily be transferred to a production-grade setting. \n",
        "\n",
        "To solve the above challenging problem, Zen ML was introduced. This has got a set of standards and well-structured abstractions. It is essential that these abstractions not only cover concepts such as pipelines and steps but also the infrastructure elements on which the pipelines run. This helps to simply infrastructure configuration and management. ZenML is a framework to create reproducible, production-ready machine learning pipelines. It is built for data scientist to transition their models from a local experimental setup to a robust modern MLOPS infrastructure in production.  "
      ],
      "metadata": {
        "id": "cV4LULRKNYRN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T__4ElNem2c"
      },
      "source": [
        "Since we will build models with [sklearn](https://scikit-learn.org/stable/), you will need to have the ZenML sklearn integration installed. You can install ZenML and the sklearn integration with the following command, which will also restart the kernel of your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hUe3ay8Oem2d",
        "outputId": "e660b327-4e59-4818-a00f-700ec3219f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting zenml\n",
            "  Downloading zenml-0.13.0-py3-none-any.whl (877 kB)\n",
            "\u001b[K     |████████████████████████████████| 877 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting rich[jupyter]<13.0.0,>=12.0.0\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 67.6 MB/s \n",
            "\u001b[?25hCollecting sqlmodel<0.1.0,>=0.0.6\n",
            "  Downloading sqlmodel-0.0.6-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from zenml) (1.9.2)\n",
            "Collecting click-params<0.4.0,>=0.3.0\n",
            "  Downloading click_params-0.3.0-py3-none-any.whl (12 kB)\n",
            "Collecting httplib2<0.20,>=0.19.1\n",
            "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting click<9.0.0,>=8.0.1\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting markupsafe==1.1.1\n",
            "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
            "Collecting apache-beam<3.0.0,>=2.30.0\n",
            "  Downloading apache_beam-2.41.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 47.1 MB/s \n",
            "\u001b[?25hCollecting ml-pipelines-sdk==1.8.0\n",
            "  Downloading ml_pipelines_sdk-1.8.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 72.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6.0.0,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 74.0 MB/s \n",
            "\u001b[?25hCollecting gitpython<4.0.0,>=3.1.18\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting distro<2.0.0,>=1.6.0\n",
            "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
            "Collecting nbconvert==6.4.4\n",
            "  Downloading nbconvert-6.4.4-py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0.0,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from zenml) (1.3.5)\n",
            "Collecting pyparsing<3,>=2.4.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from zenml) (2.8.2)\n",
            "Collecting analytics-python<2.0.0,>=1.4.0\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting ml-metadata<1.9.0,>=1.8.0\n",
            "  Downloading ml_metadata-1.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.7/dist-packages (from ml-pipelines-sdk==1.8.0->zenml) (1.2.0)\n",
            "Collecting docker<5,>=4.1\n",
            "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 30.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from ml-pipelines-sdk==1.8.0->zenml) (2.11.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.7/dist-packages (from ml-pipelines-sdk==1.8.0->zenml) (3.17.3)\n",
            "Collecting packaging<21,>=20\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.7/dist-packages (from ml-pipelines-sdk==1.8.0->zenml) (1.12.11)\n",
            "Collecting google-apitools<1,>=0.5\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from ml-pipelines-sdk==1.8.0->zenml) (1.3.9)\n",
            "Collecting nbclient<0.6.0,>=0.5.0\n",
            "  Downloading nbclient-0.5.13-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (2.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (4.6.3)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (5.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (1.5.0)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (5.4.0)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (5.1.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert==6.4.4->zenml) (4.11.1)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.7/dist-packages (from analytics-python<2.0.0,>=1.4.0->zenml) (2.23.0)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python<2.0.0,>=1.4.0->zenml) (1.15.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (1.7)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 51.2 MB/s \n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting requests<3.0,>=2.7\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2,>=1.33.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (1.47.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (2022.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (4.1.1)\n",
            "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (6.0.1)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.7.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.1.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam<3.0.0,>=2.30.0->zenml) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=8.0.1->zenml) (4.12.0)\n",
            "Collecting validators<0.19,>=0.18\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (1.31.6)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (3.0.1)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (1.35.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (0.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (1.56.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (57.4.0)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading fasteners-0.17.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from google-apitools<1,>=0.5->ml-pipelines-sdk==1.8.0->zenml) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client<2,>=1.8->ml-pipelines-sdk==1.8.0->zenml) (4.9)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting attrs<21,>=20.3\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.7/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert==6.4.4->zenml) (6.1.12)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert==6.4.4->zenml) (23.2.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert==6.4.4->zenml) (5.1.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert==6.4.4->zenml) (2.16.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert==6.4.4->zenml) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert==6.4.4->zenml) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert==6.4.4->zenml) (5.9.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.4->nbconvert==6.4.4->zenml) (3.8.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->ml-pipelines-sdk==1.8.0->zenml) (0.4.8)\n",
            "Collecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.7->analytics-python<2.0.0,>=1.4.0->zenml) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.7->analytics-python<2.0.0,>=1.4.0->zenml) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.7->analytics-python<2.0.0,>=1.4.0->zenml) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.7->analytics-python<2.0.0,>=1.4.0->zenml) (2.10)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets<8.0.0,>=7.5.1 in /usr/local/lib/python3.7/dist-packages (from rich[jupyter]<13.0.0,>=12.0.0->zenml) (7.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (7.9.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (3.0.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (5.3.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (3.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.2.5)\n",
            "Collecting sqlalchemy2-stubs\n",
            "  Downloading sqlalchemy2_stubs-0.0.2a25-py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy<1.5.0,>=1.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlmodel<0.1.0,>=0.0.6->zenml) (1.4.40)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy<1.5.0,>=1.4.17->sqlmodel<0.1.0,>=0.0.6->zenml) (1.1.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (1.8.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]<13.0.0,>=12.0.0->zenml) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert==6.4.4->zenml) (0.5.1)\n",
            "Building wheels for collected packages: dill, docopt\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=d0227b7028a781541697628e0a125d492d66947ee9990532021ae32183ad4bfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=b71f266d0c1cc664e6ca6544c3a4deee2ccb53b07f4ed847afe3fd31bdf6134f\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built dill docopt\n",
            "Installing collected packages: attrs, nest-asyncio, markupsafe, jedi, nbclient, jupyterlab-pygments, pyparsing, protobuf, nbconvert, requests, packaging, httplib2, websocket-client, smmap, fasteners, docopt, commonmark, validators, sqlalchemy2-stubs, rich, pymongo, proto-plus, orjson, monotonic, ml-metadata, hdfs, google-apitools, gitdb, fastavro, docker, dill, cloudpickle, click, backoff, sqlmodel, pyyaml, ml-pipelines-sdk, gitpython, distro, click-params, apache-beam, analytics-python, zenml\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.1.0\n",
            "    Uninstalling attrs-22.1.0:\n",
            "      Successfully uninstalled attrs-22.1.0\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: httplib2\n",
            "    Found existing installation: httplib2 0.17.4\n",
            "    Uninstalling httplib2-0.17.4:\n",
            "      Successfully uninstalled httplib2-0.17.4\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.2.0\n",
            "    Uninstalling pymongo-4.2.0:\n",
            "      Successfully uninstalled pymongo-4.2.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed analytics-python-1.4.0 apache-beam-2.41.0 attrs-20.3.0 backoff-1.10.0 click-8.1.3 click-params-0.3.0 cloudpickle-2.1.0 commonmark-0.9.1 dill-0.3.1.1 distro-1.7.0 docker-4.4.4 docopt-0.6.2 fastavro-1.6.0 fasteners-0.17.3 gitdb-4.0.9 gitpython-3.1.27 google-apitools-0.5.32 hdfs-2.7.0 httplib2-0.19.1 jedi-0.18.1 jupyterlab-pygments-0.2.2 markupsafe-1.1.1 ml-metadata-1.8.0 ml-pipelines-sdk-1.8.0 monotonic-1.6 nbclient-0.5.13 nbconvert-6.4.4 nest-asyncio-1.5.5 orjson-3.7.12 packaging-20.9 proto-plus-1.22.0 protobuf-3.20.1 pymongo-3.12.3 pyparsing-2.4.7 pyyaml-5.4.1 requests-2.28.1 rich-12.5.1 smmap-5.0.0 sqlalchemy2-stubs-0.0.2a25 sqlmodel-0.0.6 validators-0.18.2 websocket-client-1.3.3 zenml-0.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Installing integrations...\n",
            "\u001b[1A\u001b[2K\u001b[1;35mInitializing the ZenML global configuration version to 0.13.0\u001b[0m\n",
            "\u001b[1;35mCreating default profile...\u001b[0m\n",
            "\u001b[1;35mInitializing profile \u001b[0m\u001b[33mdefault\u001b[1;35m...\u001b[0m\n",
            "\u001b[1;35mRegistering default stack...\u001b[0m\n",
            "\u001b[1;35mRegistered stack component with type 'orchestrator' and name 'default'.\u001b[0m\n",
            "\u001b[1;35mRegistered stack component with type 'metadata_store' and name 'default'.\u001b[0m\n",
            "\u001b[1;35mRegistered stack component with type 'artifact_store' and name 'default'.\u001b[0m\n",
            "\u001b[1;35mRegistered stack with name 'default'.\u001b[0m\n",
            "\u001b[1;35mCreated and activated default profile.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyparsing==2.4.2\n",
            "  Downloading pyparsing-2.4.2-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyparsing\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "Successfully installed pyparsing-2.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%pip install zenml\n",
        "!zenml integration install sklearn -y\n",
        "%pip install pyparsing==2.4.2  # required for Colab\n",
        "\n",
        "import IPython\n",
        "\n",
        "# automatically restart kernel\n",
        "IPython.Application.instance().kernel.do_shutdown(restart=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlz9rV--em2i"
      },
      "source": [
        "As an ML practitioner, you are probably familiar with building ML models using Scikit-learn, PyTorch, TensorFlow, or similar. An **[ML Pipeline](https://docs.zenml.io/developer-guide/steps-and-pipelines)** is simply an extension, including other steps you would typically do before or after building a model, like data acquisition, preprocessing, model deployment, or monitoring. The ML pipeline essentially defines a step-by-step procedure of your work as an ML practitioner. Defining ML pipelines explicitly in code is great because:\n",
        "- We can easily rerun all of our work, not just the model, eliminating bugs and making our models easier to reproduce.\n",
        "- Data and models can be versioned and tracked, so we can see at a glance which dataset a model was trained on and how it compares to other models.\n",
        "- If the entire pipeline is coded up, we can automate many operational tasks, like retraining and redeploying models when the underlying problem or data changes or rolling out new and improved models with CI/CD workflows.\n",
        "\n",
        "Having a clearly defined ML pipeline is essential for ML teams that aim to serve models on a large scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTAuQH7mem2j"
      },
      "source": [
        "## ZenML Setup\n",
        "Throughout this series, we will define our ML pipelines using [ZenML](https://github.com/zenml-io/zenml/). ZenML is an excellent tool for this task, as it is straightforward and intuitive to use and has [integrations](https://docs.zenml.io/mlops-stacks/integrations) with most of the advanced MLOps tools we will want to use later. Make sure you have ZenML installed (via `pip install zenml`). Let's run some commands to make sure you start with a fresh ML stack. You can ignore the details for now, as we will learn about it in more detail in a later chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vrwm0lUYem2k"
      },
      "outputs": [],
      "source": [
        "!rm -rf .zen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize zenML repository:"
      ],
      "metadata": {
        "id": "3TdDLj4lnYb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below command will internally create a local directory with a bunch of configuration for your MLOPs stack. Stacks represent different configurations of MLOps tools and infrastructure; Each stack consists of multiple Stack Components that each come in several Flavors. The default local stack will be 'default'. This local configuration will only take effect when you’re running ZenML from the initialized repository root, or from a subdirectory. The default stack consists of  \n",
        "\n",
        "- Orchestrator: This is essentially your python kernel. \n",
        "\n",
        "- Artifact store: This store all the artifacts that flow through between steps \n",
        "\n",
        "- Metadata store: This keeps tracks of all the parameters that flow through your pipeline. \n",
        "\n",
        "Repositories link stacks to the pipeline and step code of your ML projects. "
      ],
      "metadata": {
        "id": "efaE723gwErL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zenml init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTcd4DdDnVOX",
        "outputId": "2053f2c0-db02-40d3-fcf0-a6acdf309908"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Initializing ZenML repository at /content.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /content.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /content.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /content.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mZenML repository initialized at \u001b[0m\u001b[2;35m/\u001b[0m\u001b[2;95mcontent.\u001b[0m\n",
            "\u001b[2;32m⠸\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;36mInitializing ZenML repository at /content.\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /content.\n",
            "\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mThe local active profile was initialized to \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m and the local active stack\u001b[0m\n",
            "\u001b[2;36mto \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m. This local configuration will only take effect when you're running\u001b[0m\n",
            "\u001b[2;36mZenML from the initialized repository root, or from a subdirectory. For more \u001b[0m\n",
            "\u001b[2;36minformation on profile and stack configuration, please visit \u001b[0m\n",
            "\u001b[2;4;94mhttps://docs.zenml.io/developer-guide/stacks-profiles-repositories.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profiles manage these stacks and enable having various ZenML configurations on the same machine. "
      ],
      "metadata": {
        "id": "znb0IB7ayJ8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zenml profile create zenbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvg4B7KrngaU",
        "outputId": "66742c94-9601-47bc-9a8d-a7871ad275fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2;36mRunning with active profile: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mlocal\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
            "\u001b[1;35mInitializing profile \u001b[0m\u001b[33mzenbytes\u001b[1;35m...\u001b[0m\n",
            "\u001b[1;35mRegistering default stack...\u001b[0m\n",
            "\u001b[1;35mRegistered stack component with type 'orchestrator' and name 'default'.\u001b[0m\n",
            "\u001b[1;35mRegistered stack component with type 'metadata_store' and name 'default'.\u001b[0m\n",
            "\u001b[1;35mRegistered stack component with type 'artifact_store' and name 'default'.\u001b[0m\n",
            "\u001b[1;35mRegistered stack with name 'default'.\u001b[0m\n",
            "\u001b[2;36mProfile \u001b[0m\u001b[2;32m'zenbytes'\u001b[0m\u001b[2;36m successfully created.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zenml profile set zenbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O_YxUU7oFpz",
        "outputId": "fe9b71aa-edcd-41f0-fa5a-5a4a87cd0d8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2;36mRunning with active profile: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mlocal\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
            "\u001b[?25l\u001b[2;36mActive profile changed to: \u001b[0m\u001b[2;32m'zenbytes'\u001b[0m\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Setting the active profile to 'zenbytes'...\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zenml stack set default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfJb-riFoHGd",
        "outputId": "d3d9bc91-805f-42df-d57f-a17a91864e61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2;36mRunning with active profile: \u001b[0m\u001b[2;32m'zenbytes'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mlocal\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
            "\u001b[2K\u001b[2;36mActive stack set to: \u001b[0m\u001b[2;32m'default'\u001b[0m\n",
            "\u001b[2K\u001b[32m⠼\u001b[0m Setting the active stack to 'default'...\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zenml stack get"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Jp9j9-oLTG",
        "outputId": "78fc86b1-73cf-42d0-c31d-e0c5c6da1367"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "\u001b[2;36mRunning with active profile: \u001b[0m\u001b[2;32m'zenbytes'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mlocal\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
            "\u001b[?25l\u001b[2;36mThe active stack is: \u001b[0m\u001b[2;32m'default'\u001b[0m\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Getting the active stack...\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGvNB52bem2k"
      },
      "source": [
        "## Example Experimentation ML Code\n",
        "Let us get started with some simple exemplary ML code. In the following, we train a Scikit-learn SVC classifier to classify images of handwritten digits. We load the data, train a model on the training set, then test it on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first do the import"
      ],
      "metadata": {
        "id": "vMjRX638yno4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4medzSuem2n",
        "outputId": "93620944-8a55-4899-e10a-4a96a1670160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9583333333333334\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def train_test() -> None:\n",
        "    \"\"\"Train and test a Scikit-learn SVC classifier on digits\"\"\"\n",
        "    digits = load_digits()\n",
        "    data = digits.images.reshape((len(digits.images), -1))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, digits.target, test_size=0.2, shuffle=False\n",
        "    )\n",
        "    model = SVC(gamma=0.001)\n",
        "    model.fit(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "\n",
        "train_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZXm3B5Dem2p"
      },
      "source": [
        "## Turning experiments into ML pipelines with ZenML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzEl4UZtem2q"
      },
      "source": [
        "In ZenML, all the things can be defined as functions using the functional API. All you must do is to define a function, define its inputs, define its output, and then write python code in the middle. You just need to decorate that function with step decorator which you import from ZenML. Steps are the atomic components of a ZenML pipeline. Each step is defined by its inputs, the logic it applies and its outputs. \n",
        "\n",
        "For simple illustrations, assume your ML workflow contains data loading, model training, and model evaluation. In practice, your ML workflows will, of course, be much more complicated than that. You might have complex preprocessing that you do not want to redo every time you train a model, you will need to compare the performance of different models, deploy them in a production setting, and much more. Here ML pipelines come into play, allowing us to define our workflows in modular steps that we can then mix and match.\n",
        "\n",
        "![Digits Pipeline](https://github.com/zenml-io/zenbytes/blob/main/_assets/1-1/digits_pipeline.png?raw=1)\n",
        "\n",
        "We can identify three distinct steps in our example: data loading, model training, and model evaluation. Let us now define each of them as a ZenML **[Pipeline Step](https://docs.zenml.io/developer-guide/steps-and-pipelines#step)** simply by moving each step to its own function and decorating them with ZenML's `@step` [Python decorator](https://realpython.com/primer-on-python-decorators/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps are the atomic components of a ZenML pipeline. Each step is defined by its inputs, the logic it applies and its outputs. "
      ],
      "metadata": {
        "id": "He46hRVzJUu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q3JHyD7Aem2r"
      },
      "outputs": [],
      "source": [
        "from zenml.steps import step, Output\n",
        "\n",
        "@step\n",
        "def importer() -> Output(\n",
        "    X_train=np.ndarray,\n",
        "    X_test=np.ndarray,\n",
        "    y_train=np.ndarray,\n",
        "    y_test=np.ndarray,\n",
        "):\n",
        "    \"\"\"Load the digits dataset as numpy arrays.\"\"\"\n",
        "    digits = load_digits()\n",
        "    data = digits.images.reshape((len(digits.images), -1))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, digits.target, test_size=0.2, shuffle=False\n",
        "    )\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this step has multiple outputs, we need to use the zenml.steps.step_output.Output class to indicate the names of each output. These names can be used to directly access the outputs of steps after running a pipeline."
      ],
      "metadata": {
        "id": "3RvUiLsaKLMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's come up with a second step that consumes the output of our first step and performs some sort of transformation on it. In this case, let's train a support vector machine classifier on the training data using sklearn:"
      ],
      "metadata": {
        "id": "amGFo6aqKh-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@step\n",
        "def svc_trainer(\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        ") -> ClassifierMixin:\n",
        "    \"\"\"Train a sklearn SVC classifier.\"\"\"\n",
        "    model = SVC(gamma=0.001)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "j_Thq5L4KIK5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@step\n",
        "def evaluator(\n",
        "    X_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    model: ClassifierMixin,\n",
        ") -> float:\n",
        "    \"\"\"Calculate the test set accuracy of an sklearn model.\"\"\"\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    print(f\"Test accuracy: {test_acc}\")\n",
        "    return test_acc"
      ],
      "metadata": {
        "id": "XTYLbjJTKKBb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn1rxYIIem26"
      },
      "source": [
        "Similarly, we can use ZenML's `@pipeline` decorator to connect all of our steps into an ML pipeline.This is agnostic of the implementation and can be done by routing outputs through the steps within the pipeline.\n",
        "\n",
        "Note that the pipeline definition does not depend on the concrete step functions we defined above; it merely establishes a recipe for how data moves through the steps. This means we can replace steps as we wish, e.g., to run the same pipeline with different models to compare their performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rFyQ5rCwem27"
      },
      "outputs": [],
      "source": [
        "from zenml.pipelines import pipeline\n",
        "\n",
        "\n",
        "@pipeline\n",
        "def digits_pipeline(importer, trainer, evaluator):\n",
        "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
        "    X_train, X_test, y_train, y_test = importer()\n",
        "    model = trainer(X_train=X_train, y_train=y_train)\n",
        "    evaluator(X_test=X_test, y_test=y_test, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you want to run the step function outside the context of a ZenML pipeline, all you need to do is call the .entrypoint() method with the same input signature. For example:\n",
        "trainer.entrypoint(X_train=..., y_train=...)"
      ],
      "metadata": {
        "id": "4D_rUP_rKwxb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8bSh_-em28"
      },
      "source": [
        "## Running ZenML Pipelines\n",
        "Finally, we initialize our pipeline with concrete step functions and call the `run()` method to run it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With your pipeline recipe in hand you can now specify which concrete step implementations to use when instantiating the pipeline:"
      ],
      "metadata": {
        "id": "Mf4B0nssMBJF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LOwwhjTEem28"
      },
      "outputs": [],
      "source": [
        "digits_svc_pipeline = digits_pipeline(\n",
        "    importer=importer(), trainer=svc_trainer(), evaluator=evaluator()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, you cannot use the same step twice in a pipeline because step names must be unique. If you would like to reuse a step, use the clone_step() utility function to create a copy of the step with a new name.\n",
        "\n",
        "To give each pipeline run a name:\n",
        "When running a pipeline by calling my_pipeline.run(), ZenML uses the current date and time as the name for the pipeline run. In order to change the name for a run, pass run_name as a parameter to the run() function:\n",
        "\n",
        "pipeline_instance.run(run_name=\"custom_pipeline_run_name\")\n",
        "\n",
        "Pipeline run names must be unique, so make sure to compute it dynamically if you plan to run your pipeline multiple times.\n",
        "\n",
        "You can then execute your pipeline instance with the .run() method:"
      ],
      "metadata": {
        "id": "jiJ0TrRoMG5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits_svc_pipeline.run()"
      ],
      "metadata": {
        "id": "Y6ud2W3XL44A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f843bd71-3a48-470b-fb04-99bc4348c46e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;35mCreating run for pipeline: \u001b[0m\u001b[33mdigits_pipeline\u001b[1;35m\u001b[0m\n",
            "\u001b[1;35mCache enabled for pipeline \u001b[0m\u001b[33mdigits_pipeline\u001b[1;35m\u001b[0m\n",
            "\u001b[1;35mUsing stack \u001b[0m\u001b[33mdefault\u001b[1;35m to run pipeline \u001b[0m\u001b[33mdigits_pipeline\u001b[1;35m...\u001b[0m\n",
            "\u001b[1;35mStep \u001b[0m\u001b[33mimporter\u001b[1;35m has started.\u001b[0m\n",
            "\u001b[1;35mStep \u001b[0m\u001b[33mimporter\u001b[1;35m has finished in 0.285s.\u001b[0m\n",
            "\u001b[1;35mStep \u001b[0m\u001b[33msvc_trainer\u001b[1;35m has started.\u001b[0m\n",
            "\u001b[1;35mStep \u001b[0m\u001b[33msvc_trainer\u001b[1;35m has finished in 0.231s.\u001b[0m\n",
            "\u001b[1;35mStep \u001b[0m\u001b[33mevaluator\u001b[1;35m has started.\u001b[0m\n",
            "Test accuracy: 0.9583333333333334\n",
            "\u001b[1;35mStep \u001b[0m\u001b[33mevaluator\u001b[1;35m has finished in 0.113s.\u001b[0m\n",
            "\u001b[1;35mPipeline run \u001b[0m\u001b[33mdigits_pipeline-24_Aug_22-09_40_42_106976\u001b[1;35m has finished in 0.826s.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5pTnywxem29"
      },
      "source": [
        "And that's it, we just built our first ML pipeline! Great job!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit ('zenbytes')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9f70ec6e6bd16014ded89c8222361cbe53cd9507d51ebdcdf3ab6e494d45cf74"
      }
    },
    "colab": {
      "provenance": [],
      "name": "ML_Pipeline_with_ZenML.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}